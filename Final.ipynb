{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a89cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache loaded from disk.\n",
      "Cache saved to disk.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import threading\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "class CombinedCache:\n",
    "    def __init__(self, maxsize=10000, ttl=300):\n",
    "        self.user_cache = defaultdict(lambda: None)\n",
    "        self.tweet_cache = defaultdict(lambda: None)\n",
    "        self.timestamps = defaultdict(lambda: 0)  # Track last access time for TTL\n",
    "        self.maxsize = maxsize\n",
    "        self.ttl = ttl\n",
    "\n",
    "    def get_user(self, user_id):\n",
    "        return self.user_cache[user_id]\n",
    "\n",
    "    def get_tweet(self, tweet_id):\n",
    "        tweet = self.tweet_cache[tweet_id]\n",
    "        if tweet and (time.time() - self.timestamps[tweet_id] < self.ttl):\n",
    "            return tweet\n",
    "        return None\n",
    "\n",
    "    def put_user(self, user_id, user_data):\n",
    "        self.user_cache[user_id] = user_data\n",
    "\n",
    "    def put_tweet(self, tweet_id, tweet_data):\n",
    "        self.tweet_cache[tweet_id] = tweet_data\n",
    "        self.timestamps[tweet_id] = time.time()\n",
    "\n",
    "    def remove_user(self, user_id):\n",
    "        if user_id in self.user_cache:\n",
    "            del self.user_cache[user_id]\n",
    "\n",
    "    def remove_tweet(self, tweet_id):\n",
    "        if tweet_id in self.tweet_cache:\n",
    "            del self.tweet_cache[tweet_id]\n",
    "            del self.timestamps[tweet_id]\n",
    "\n",
    "    def save_to_disk(self, filename='cache.json'):\n",
    "        data = {\n",
    "            'user_cache': dict(self.user_cache),\n",
    "            'tweet_cache': dict(self.tweet_cache),\n",
    "            'timestamps': dict(self.timestamps)\n",
    "        }\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        print(\"Cache saved to disk.\")\n",
    "\n",
    "    def load_from_disk(self, filename='cache.json'):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.user_cache = defaultdict(lambda: None, data['user_cache'])\n",
    "                self.tweet_cache = defaultdict(lambda: None, data['tweet_cache'])\n",
    "                self.timestamps = defaultdict(lambda: 0, data['timestamps'])\n",
    "            print(\"Cache loaded from disk.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No cache file found, starting with an empty cache.\")\n",
    "\n",
    "# 创建缓存实例\n",
    "combined_cache = CombinedCache()\n",
    "\n",
    "# 设置周期性保存缓存\n",
    "def periodic_save_cache():\n",
    "    combined_cache.save_to_disk()\n",
    "    threading.Timer(3600, periodic_save_cache).start()  # 每小时保存一次\n",
    "\n",
    "# 在程序启动时加载缓存\n",
    "combined_cache.load_from_disk()\n",
    "periodic_save_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example for datastructrue in firestore\n",
    "{\n",
    "  \"id_str\": \"123456\",\n",
    "  \"text\": \"This is a sample tweet\",\n",
    "  \"created_at\": \"2021-01-01T12:00:00Z\",\n",
    "  \"user_id\": \"78910\",  // 只存储用户ID\n",
    "  \"retweets\": {\n",
    "    \"count\": 100,\n",
    "    \"users\": [\n",
    "      \"user_id1\",\n",
    "      \"user_id2\"\n",
    "    ]\n",
    "  },\n",
    "  \"hashtags\": [\"#example\", \"#sample\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95ef1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# 初始化 Firebase 应用\n",
    "cred = credentials.Certificate('D:/Download/twitter-a3b9a-firebase-adminsdk-b9pvo-f8f057cf01.json')\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_twitter_date(datestr):\n",
    "    return datetime.strptime(datestr, '%a %b %d %H:%M:%S %z %Y').astimezone(pytz.utc)\n",
    "\n",
    "def main():\n",
    "    tweets_seen = set()  # 跟踪已处理的推文ID\n",
    "\n",
    "    with open(\"corona-out-3\", \"r\") as f1:\n",
    "        batch = db.batch()  # 创建一个Firestore批处理操作\n",
    "        count = 0  # 计数批处理中的文档数量\n",
    "\n",
    "        for line in f1:\n",
    "            line = line.strip()\n",
    "            if not line or not line.startswith('{'):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                tweet_id = data.get('id_str')\n",
    "\n",
    "                if tweet_id in tweets_seen or tweet_id is None:\n",
    "                    continue\n",
    "\n",
    "                tweets_seen.add(tweet_id)\n",
    "                user_id = data['user'].get('id_str')\n",
    "\n",
    "                # 准备 Firestore 的推文数据\n",
    "                tweet_data = {\n",
    "                    'id_str': tweet_id,\n",
    "                    'text': data['text'],\n",
    "                    'created_at': data['created_at'],\n",
    "                    'user_id': user_id,\n",
    "                    'like_count': data.get('favorite_count', 0),\n",
    "                    'retweet_count': data.get('retweet_count', 0),\n",
    "                    'reply_count': data.get('reply_count', 0),\n",
    "                    'hashtags': [tag['text'] for tag in data.get('entities', {}).get('hashtags', [])]\n",
    "                }\n",
    "                tweet_ref = db.collection('tweets').document(str(tweet_id))\n",
    "                batch.set(tweet_ref, tweet_data)\n",
    "\n",
    "                # 检查是否存在转发状态\n",
    "                if 'retweeted_status' in data:\n",
    "                    retweeted = data['retweeted_status']\n",
    "                    retweet_data = {\n",
    "                        'original_tweet_id': tweet_id,\n",
    "                        'retweeter_id': retweeted['user']['id_str'],\n",
    "                        'retweet_time': retweeted['created_at'],\n",
    "                        'retweet_text': retweeted.get('extended_tweet', {}).get('full_text', retweeted['text']) if 'extended_tweet' in retweeted else retweeted['text']\n",
    "                    }\n",
    "                    retweet_ref = db.collection('retweets').document(tweet_id + '_' + retweeted['user']['id_str'])\n",
    "                    batch.set(retweet_ref, retweet_data)\n",
    "\n",
    "                count += 2  # 更新两个文档\n",
    "                if count >= 400:  # Firestore批处理限制在一次操作中最多可以处理500个操作\n",
    "                    batch.commit()  # 提交批处理\n",
    "                    batch = db.batch()  # 重置批处理\n",
    "                    count = 0\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line: {e}\")\n",
    "                continue\n",
    "\n",
    "        if count > 0:\n",
    "            batch.commit()  # 提交最后一批剩余的数据\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a6c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5cd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 \n",
    "#postgre storage\n",
    "# 数据库连接和其他初始化\n",
    "host = \"localhost\"\n",
    "dbname = \"twitter_1\"\n",
    "user = \"postgres\"\n",
    "password = \"123456\"\n",
    "port = \"5432\"\n",
    "conn = psycopg2.connect(host=host, dbname=dbname, user=user, password=password, port=port)\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 创建用户表\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS user_table (\n",
    "    id BIGINT PRIMARY KEY,\n",
    "    id_str VARCHAR(50) NOT NULL,\n",
    "    name VARCHAR(255),\n",
    "    screen_name VARCHAR(255),\n",
    "    location VARCHAR(255),\n",
    "    url VARCHAR(500),\n",
    "    description TEXT,\n",
    "    translator_type VARCHAR(50),\n",
    "    protected BOOLEAN,\n",
    "    verified BOOLEAN,\n",
    "    followers_count INT,\n",
    "    friends_count INT,\n",
    "    listed_count INT,\n",
    "    favourites_count INT,\n",
    "    statuses_count INT,\n",
    "    created_at TIMESTAMP WITHOUT TIME ZONE,\n",
    "    utc_offset INT,\n",
    "    time_zone VARCHAR(50),\n",
    "    geo_enabled BOOLEAN,\n",
    "    lang VARCHAR(50),\n",
    "    contributors_enabled BOOLEAN,\n",
    "    is_translator BOOLEAN,\n",
    "    profile_background_color VARCHAR(7),\n",
    "    profile_background_image_url VARCHAR(500),\n",
    "    profile_background_image_url_https VARCHAR(500),\n",
    "    profile_background_tile BOOLEAN,\n",
    "    profile_link_color VARCHAR(7),\n",
    "    profile_sidebar_border_color VARCHAR(7),\n",
    "    profile_sidebar_fill_color VARCHAR(7),\n",
    "    profile_text_color VARCHAR(7),\n",
    "    profile_use_background_image BOOLEAN,\n",
    "    profile_image_url VARCHAR(500),\n",
    "    profile_image_url_https VARCHAR(500),\n",
    "    profile_banner_url VARCHAR(500),\n",
    "    default_profile BOOLEAN,\n",
    "    default_profile_image BOOLEAN,\n",
    "    following BOOLEAN,\n",
    "    follow_request_sent BOOLEAN,\n",
    "    notifications BOOLEAN\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.execute(create_table_query)\n",
    "    print(\"Table created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# 读取文件并处理每行\n",
    "file_path = \"corona-out-3\"  # 确保这是正确的文件路径\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line_number, line in enumerate(file, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            print(f\"Skipping empty line: {line_number}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            user_id_str = data['user']['id_str']\n",
    "            user_data = data['user']\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON on line {line_number}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # 尝试从缓存获取用户信息\n",
    "        cached_user = cache.get(user_id_str)\n",
    "        if cached_user:\n",
    "            print(f\"User {user_id_str} already exists in cache, skipping insert.\")\n",
    "            continue\n",
    "        \n",
    "        # 查询数据库以确认用户是否存在\n",
    "        cursor.execute(\"SELECT * FROM user_table WHERE id_str = %s;\", (user_id_str,))\n",
    "        result = cursor.fetchone()\n",
    "        \n",
    "        if not result:\n",
    "            try:\n",
    "                # 将用户数据插入数据库\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO user_table (id, id_str, name, screen_name, location, url, description, protected, verified, followers_count, friends_count, listed_count, favourites_count, statuses_count, created_at, profile_background_color, profile_link_color, profile_sidebar_border_color, profile_sidebar_fill_color, profile_text_color, profile_use_background_image, profile_image_url, profile_image_url_https, profile_banner_url, default_profile, default_profile_image, following, follow_request_sent, notifications)\n",
    "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "                \"\"\", (\n",
    "                    user_data['id'], user_data['id_str'], user_data['name'], user_data['screen_name'],\n",
    "                    user_data['location'], user_data.get('url'), user_data['description'],\n",
    "                    user_data['protected'], user_data['verified'], user_data['followers_count'],\n",
    "                    user_data['friends_count'], user_data['listed_count'], user_data['favourites_count'],\n",
    "                    user_data['statuses_count'], user_data['created_at'], user_data.get('profile_background_color'),\n",
    "                    user_data.get('profile_link_color'), user_data.get('profile_sidebar_border_color'),\n",
    "                    user_data.get('profile_sidebar_fill_color'), user_data.get('profile_text_color'),\n",
    "                    user_data.get('profile_use_background_image'), user_data.get('profile_image_url'),\n",
    "                    user_data.get('profile_image_url_https'), user_data.get('profile_banner_url'),\n",
    "                    user_data.get('default_profile'), user_data.get('default_profile_image'),\n",
    "                    user_data.get('following'), user_data.get('follow_request_sent'), user_data.get('notifications')\n",
    "                ))\n",
    "                print(f\"User {user_id_str} inserted into the database.\")\n",
    "                # 添加用户信息到缓存\n",
    "                cache.put(user_id_str, user_data)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error inserting data on line {line_number}: {ex}\")\n",
    "        else:\n",
    "            print(f\"User {user_id_str} already exists in the database, skipping insert.\")\n",
    "\n",
    "# 程序结束前保存缓存状态\n",
    "cache.checkpoint_cache()\n",
    "# 关闭游标和连接\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f392854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abf8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox, ttk, scrolledtext\n",
    "import psycopg2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8187d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance(tweet, query):\n",
    "    # 基础分数：文本匹配度（简单示例，实际应用中可能需要更复杂的文本分析）\n",
    "    text_score = 1 if query.lower() in tweet['text'].lower() else 0\n",
    "\n",
    "    # 互动分数\n",
    "    interaction_score = (tweet.get('like_count', 0) * 0.1 +\n",
    "                         tweet.get('retweet_count', 0) * 0.2 +\n",
    "                         tweet.get('reply_count', 0) * 0.1)\n",
    "\n",
    "    # 用户影响力分数\n",
    "    influence_score = tweet.get('user', {}).get('follower_count', 0) * 0.001  # 根据需要调整权重\n",
    "\n",
    "    # 综合评分\n",
    "    return text_score * 0.5 + interaction_score * 0.3 + influence_score * 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox, ttk, scrolledtext\n",
    "import psycopg2 \n",
    "def parse_twitter_date(datestr):\n",
    "    # 示例 datestr: \"Sat Apr 25 12:21:41 +0000 2020\"\n",
    "    return datetime.strptime(datestr, '%a %b %d %H:%M:%S %z %Y').astimezone(pytz.utc)\n",
    "\n",
    "def search_tweets(query, search_type=\"text\", start_date=None, end_date=None, sort_by=\"created_at\", order=\"DESC\"):\n",
    "    db = firestore.client()\n",
    "    tweets = db.collection('tweets')\n",
    "\n",
    "    if search_type == \"text\":\n",
    "        tweets = tweets.where('text', '>=', query).where('text', '<=', query + '\\uf8ff')\n",
    "    elif search_type == \"hashtag\":\n",
    "        tweets = tweets.where('hashtags', 'array_contains', query)\n",
    "\n",
    "    if start_date and end_date:\n",
    "        # 为用户输入的日期添加 UTC 时区信息\n",
    "        utc_zone = pytz.utc\n",
    "        start_date = utc_zone.localize(datetime.strptime(start_date, \"%Y-%m-%d\"))\n",
    "        end_date = utc_zone.localize(datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1))\n",
    "        \n",
    "        filtered_results = []\n",
    "        for doc in tweets.stream():\n",
    "            tweet = doc.to_dict()\n",
    "            tweet_created_at = parse_twitter_date(tweet['created_at'])\n",
    "            if start_date <= tweet_created_at <= end_date:\n",
    "                filtered_results.append(tweet)\n",
    "        results = filtered_results\n",
    "    else:\n",
    "        results = [doc.to_dict() for doc in tweets.stream()]\n",
    "\n",
    "    tweets = tweets.order_by(sort_by, direction=firestore.Query.DESCENDING if order == \"DESC\" else firestore.Query.ASCENDING)\n",
    "    try:\n",
    "        results = [doc.to_dict() for doc in tweets.stream()]\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to fetch tweets: {e}\")\n",
    "        return []\n",
    "\n",
    "    user_ids = {result['user_id'] for result in results}\n",
    "    user_data = get_user_data(list(user_ids)) if user_ids else {}\n",
    "    \n",
    "    for result in results:\n",
    "        result['user'] = user_data.get(result['user_id'], {})\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_user_data(user_ids):\n",
    "    if not user_ids:\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            dbname=\"twitter_1\",\n",
    "            user=\"postgres\",\n",
    "            password=\"123456\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        format_strings = ','.join(['%s'] * len(user_ids))\n",
    "        cursor.execute(f\"SELECT id, name, screen_name, location, url FROM user_table WHERE id IN ({format_strings})\", tuple(user_ids))\n",
    "        user_data = {row[0]: {'name': row[1], 'screen_name': row[2], 'location': row[3], 'url': row[4]} for row in cursor.fetchall()}\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return user_data\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to fetch user data: {e}\")\n",
    "        return {}\n",
    "\n",
    "def perform_search():\n",
    "    search_query = query_entry.get()\n",
    "    search_type = search_type_combobox.get()\n",
    "    start_date = start_date_entry.get()\n",
    "    end_date = end_date_entry.get()\n",
    "    results = search_tweets(search_query, search_type, start_date, end_date)\n",
    "    results_display.delete(1.0, tk.END)\n",
    "    \n",
    "    for index, result in enumerate(results):\n",
    "        user_info = result.get('user', {})\n",
    "        result_text = f\"{result['text']} - {user_info.get('name', 'Unknown')} - {result.get('created_at', 'No date')}\\n\"\n",
    "        results_display.insert(tk.END, result_text)\n",
    "        details_button = tk.Button(results_display, text=\"Details\", command=lambda r=result: show_details(r))\n",
    "        results_display.window_create(tk.END, window=details_button)\n",
    "        results_display.insert(tk.END, \"\\n\")\n",
    "\n",
    "def show_details(result):\n",
    "    user_info = result.get('user', {})\n",
    "    details = f\"Tweet ID: {result.get('id_str', 'N/A')}\\nUser: {user_info.get('name', 'Unknown')}\\nScreen Name: {user_info.get('screen_name', 'N/A')}\\nLocation: {user_info.get('location', 'N/A')}\\nText: {result['text']}\\nHashtags: {', '.join(result.get('hashtags', []))}\"\n",
    "    messagebox.showinfo(\"Tweet Details\", details)\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Twitter Search App\")\n",
    "\n",
    "# 创建界面元素\n",
    "query_label = tk.Label(root, text=\"Enter search query:\")\n",
    "query_label.pack()\n",
    "query_entry = tk.Entry(root)\n",
    "query_entry.pack()\n",
    "\n",
    "search_type_label = tk.Label(root, text=\"Search by:\")\n",
    "search_type_label.pack()\n",
    "search_type_combobox = ttk.Combobox(root, values=(\"text\", \"hashtag\", \"user\"))\n",
    "search_type_combobox.pack()\n",
    "search_type_combobox.current(0)\n",
    "\n",
    "start_date_label = tk.Label(root, text=\"Start Date (YYYY-MM-DD):\")\n",
    "start_date_label.pack()\n",
    "start_date_entry = tk.Entry(root)\n",
    "start_date_entry.pack()\n",
    "\n",
    "end_date_label = tk.Label(root, text=\"End Date (YYYY-MM-DD):\")\n",
    "end_date_label.pack()\n",
    "end_date_entry = tk.Entry(root)\n",
    "end_date_entry.pack()\n",
    "\n",
    "search_button = tk.Button(root, text=\"Search\", command=perform_search)\n",
    "search_button.pack()\n",
    "\n",
    "results_display = scrolledtext.ScrolledText(root, wrap=tk.WORD, height=10, width=50)\n",
    "results_display.pack(pady=20)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f56b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41512d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRUCache:\n",
    "    def __init__(self, capacity: int = 100):\n",
    "        self.cache = OrderedDict()\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def get(self, key):\n",
    "        if key not in self.cache:\n",
    "            return None\n",
    "        else:\n",
    "            self.cache.move_to_end(key)\n",
    "            return self.cache[key]\n",
    "\n",
    "    def put(self, key, value):\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        self.cache[key] = value\n",
    "        if len(self.cache) > self.capacity:\n",
    "            self.cache.popitem(last=False)\n",
    "\n",
    "    def save_to_disk(self):\n",
    "        with open('cache.pkl', 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "    def load_from_disk(self):\n",
    "        try:\n",
    "            with open('cache.pkl', 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            self.cache = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cefaf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = credentials.Certificate('D:/Download/twitter-a3b9a-firebase-adminsdk-b9pvo-f8f057cf01.json')\n",
    "db = firestore.client()\n",
    "\n",
    "# Global Cache\n",
    "cache = LRUCache()\n",
    "cache.load_from_disk()\n",
    "\n",
    "def parse_twitter_date(datestr):\n",
    "    timezone = pytz.timezone(\"UTC\")  # Define your timezone\n",
    "    return datetime.strptime(datestr, '%a %b %d %H:%M:%S %z %Y').astimezone(timezone)\n",
    "\n",
    "def calculate_relevance(tweet, query):\n",
    "    text_score = query.lower() in tweet['text'].lower()\n",
    "    interaction_score = tweet.get('like_count', 0) + tweet.get('retweet_count', 0) + tweet.get('reply_count', 0)\n",
    "    return text_score * 0.5 + interaction_score * 0.5\n",
    "\n",
    "def get_user_data(user_ids, use_cache=True):\n",
    "    results = {}\n",
    "    missing_ids = []\n",
    "    for user_id in user_ids:\n",
    "        if use_cache:\n",
    "            data = cache.get(user_id)\n",
    "            if data:\n",
    "                results[user_id] = data\n",
    "                continue\n",
    "        missing_ids.append(user_id)\n",
    "    \n",
    "    if missing_ids:\n",
    "        try:\n",
    "            conn = psycopg2.connect(host=\"localhost\", dbname=\"twitter_1\", user=\"postgres\", password=\"123456\", port=\"5432\")\n",
    "            cursor = conn.cursor()\n",
    "            format_strings = ','.join(['%s'] * len(missing_ids))\n",
    "            cursor.execute(f\"SELECT id, name, screen_name, location, url FROM user_table WHERE id IN ({format_strings})\", tuple(missing_ids))\n",
    "            for row in cursor.fetchall():\n",
    "                user_data = {'name': row[1], 'screen_name': row[2], 'location': row[3], 'url': row[4]}\n",
    "                results[row[0]] = user_data\n",
    "                if use_cache:\n",
    "                    cache.put(row[0], user_data)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"连接数据库失败: {e}\")\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_retweet_details(tweet_id):\n",
    "    retweets = db.collection('retweets').where('original_tweet_id', '==', tweet_id).stream()\n",
    "    user_ids = [retweet.to_dict()['retweeter_id'] for retweet in retweets]\n",
    "    users_info = get_user_data(user_ids)\n",
    "    retweet_info = []\n",
    "    for user_id in user_ids:\n",
    "        if user_id in users_info:\n",
    "            user_info = users_info[user_id]\n",
    "            retweet_info.append(f\"Retweeted by: {user_info['name']} at {user_info.get('retweet_time', 'Unknown')}\")\n",
    "        else:\n",
    "            retweet_info.append(\"Retweet details unavailable\")\n",
    "    return \"\\n\".join(retweet_info)\n",
    "\n",
    "def search_tweets(query, search_type=\"text\", start_date=None, end_date=None, use_cache=True):\n",
    "    tweets = db.collection('tweets')\n",
    "    if search_type == \"text\":\n",
    "        tweets = tweets.where('text', '>=', query).where('text', '<=', query + '\\uf8ff')\n",
    "    elif search_type == \"hashtag\":\n",
    "        tweets = tweets.where('hashtags', 'array_contains', query)\n",
    "    results = []\n",
    "    if start_date and end_date:\n",
    "        utc_zone = pytz.utc\n",
    "        start_date = utc_zone.localize(datetime.strptime(start_date, \"%Y-%m-%d\"))\n",
    "        end_date = utc_zone.localize(datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1))\n",
    "        for doc in tweets.stream():\n",
    "            tweet = doc.to_dict()\n",
    "            tweet_created_at = parse_twitter_date(tweet['created_at'])\n",
    "            if start_date <= tweet_created_at <= end_date:\n",
    "                tweet['relevance_score'] = calculate_relevance(tweet, query)\n",
    "                results.append(tweet)\n",
    "        results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "    else:\n",
    "        results = [doc.to_dict() for doc in tweets.stream()]\n",
    "    user_ids = {result['user_id'] for result in results}\n",
    "    users_info = get_user_data(list(user_ids), use_cache=use_cache)  # 使用use_cache参数\n",
    "    for result in results:\n",
    "        result['user'] = users_info.get(result['user_id'], {})\n",
    "        result['retweet_details'] = get_retweet_details(result['id_str'], use_cache=use_cache)  # 确保这里也传递了use_cache参数\n",
    "    return results\n",
    "\n",
    "\n",
    "def perform_search():\n",
    "    search_query = query_entry.get()\n",
    "    search_type = search_type_combobox.get()\n",
    "    start_date = start_date_entry.get()\n",
    "    end_date = end_date_entry.get()\n",
    "    results = search_tweets(search_query, search_type, start_date, end_date)\n",
    "    results_display.delete(1.0, tk.END)\n",
    "    if not results:\n",
    "        messagebox.showinfo(\"Search Result\", \"No tweets found matching your criteria.\")\n",
    "        return\n",
    "    for index, result in enumerate(results):\n",
    "        user_info = result.get('user', {})\n",
    "        result_text = f\"{result['text']} - {user_info.get('name', 'Unknown')} - {result.get('created_at', 'No date')} - Retweets: {result.get('retweet_details', 'None')}\\n\"\n",
    "        results_display.insert(tk.END, result_text)\n",
    "        details_button = tk.Button(results_display, text=\"Details\", command=lambda r=result: show_details(r))\n",
    "        results_display.window_create(tk.END, window=details_button)\n",
    "        results_display.insert(tk.END, \"\\n\")\n",
    "\n",
    "def show_details(result):\n",
    "    user_info = result.get('user', {})\n",
    "    retweet_details = result.get('retweet_details', 'No retweet details available')\n",
    "    details = f\"Tweet ID: {result.get('id_str', 'N/A')}\\nUser: {user_info.get('name', 'Unknown')}\\nScreen Name: {user_info.get('screen_name', 'N/A')}\\nLocation: {user_info.get('location', 'N/A')}\\nText: {result['text']}\\nHashtags: {', '.join(result.get('hashtags', []))}\\nRetweet Details: {retweet_details}\"\n",
    "    messagebox.showinfo(\"Tweet Details\", details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# UI initialization and main loop\n",
    "root = tk.Tk()\n",
    "root.title(\"Twitter Search App\")\n",
    "query_label = tk.Label(root, text=\"Enter search query:\")\n",
    "query_label.pack()\n",
    "query_entry = tk.Entry(root)\n",
    "query_entry.pack()\n",
    "search_type_label = tk.Label(root, text=\"Search by:\")\n",
    "search_type_label.pack()\n",
    "search_type_combobox = ttk.Combobox(root, values=(\"text\", \"hashtag\", \"user\"))\n",
    "search_type_combobox.pack()\n",
    "search_type_combobox.current(0)\n",
    "start_date_label = tk.Label(root, text=\"Start Date (YYYY-MM-DD):\")\n",
    "start_date_label.pack()\n",
    "start_date_entry = tk.Entry(root)\n",
    "start_date_entry.pack()\n",
    "end_date_label = tk.Label(root, text=\"End Date (YYYY-MM-DD):\")\n",
    "end_date_label.pack()\n",
    "end_date_entry = tk.Entry(root)\n",
    "end_date_entry.pack()\n",
    "search_button = tk.Button(root, text=\"Search\", command=perform_search)\n",
    "search_button.pack()\n",
    "results_display = scrolledtext.ScrolledText(root, wrap=tk.WORD, height=10, width=50)\n",
    "results_display.pack(pady=20)\n",
    "\n",
    "def on_closing():\n",
    "    cache.save_to_disk()\n",
    "    root.destroy()\n",
    "\n",
    "root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4ea5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141283a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796665f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "Timeout of 300.0s exceeded, last exception: 429 Quota exceeded.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\api_core\\grpc_helpers.py:170\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m     prefetch_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callable_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_prefetch_first_result_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_StreamingResponseIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefetch_first_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefetch_first\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\api_core\\grpc_helpers.py:92\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__init__\u001b[1;34m(self, wrapped, prefetch_first_result)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefetch_first_result:\n\u001b[1;32m---> 92\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stored_first_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# It is possible the wrapped method isn't an iterable (a grpc.Call\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# for instance). If this happens don't store the first result.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\grpc\\_channel.py:542\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\grpc\\_channel.py:968\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Quota exceeded.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.81.234:443 {created_time:\"2024-04-24T13:03:44.0783445+00:00\", grpc_status:8, grpc_message:\"Quota exceeded.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\api_core\\grpc_helpers.py:174\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 Quota exceeded.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m平均查询时间（\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m启用\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m禁用\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m缓存）: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 秒\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 首次运行测试，启用缓存\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mrun_test_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 清除缓存，然后再次运行测试，禁用缓存\u001b[39;00m\n\u001b[0;32m     23\u001b[0m cache\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mclear()\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mrun_test_queries\u001b[1;34m(use_cache)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query, query_type, start_date, end_date \u001b[38;5;129;01min\u001b[39;00m test_queries:\n\u001b[0;32m     10\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 11\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     13\u001b[0m     times\u001b[38;5;241m.\u001b[39mappend(end_time \u001b[38;5;241m-\u001b[39m start_time)\n",
      "Cell \u001b[1;32mIn[7], line 71\u001b[0m, in \u001b[0;36msearch_tweets\u001b[1;34m(query, search_type, start_date, end_date, use_cache)\u001b[0m\n\u001b[0;32m     69\u001b[0m start_date \u001b[38;5;241m=\u001b[39m utc_zone\u001b[38;5;241m.\u001b[39mlocalize(datetime\u001b[38;5;241m.\u001b[39mstrptime(start_date, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     70\u001b[0m end_date \u001b[38;5;241m=\u001b[39m utc_zone\u001b[38;5;241m.\u001b[39mlocalize(datetime\u001b[38;5;241m.\u001b[39mstrptime(end_date, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tweets\u001b[38;5;241m.\u001b[39mstream():\n\u001b[0;32m     72\u001b[0m     tweet \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     73\u001b[0m     tweet_created_at \u001b[38;5;241m=\u001b[39m parse_twitter_date(tweet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\cloud\\firestore_v1\\query.py:320\u001b[0m, in \u001b[0;36mQuery.stream\u001b[1;34m(self, transaction, retry, timeout)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    286\u001b[0m     transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m     retry: retries\u001b[38;5;241m.\u001b[39mRetry \u001b[38;5;241m=\u001b[39m gapic_v1\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[0;32m    288\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    289\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[document\u001b[38;5;241m.\u001b[39mDocumentSnapshot, Any, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;124;03m\"\"\"Read the documents in the collection that match this query.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    This sends a ``RunQuery`` RPC and then returns an iterator which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m        The next document that fulfills the query.\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m     response_iterator, expected_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_stream_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransaction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     last_snapshot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\cloud\\firestore_v1\\query.py:223\u001b[0m, in \u001b[0;36mQuery._get_stream_iterator\u001b[1;34m(self, transaction, retry, timeout)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m\"\"\"Helper method for :meth:`stream`.\"\"\"\u001b[39;00m\n\u001b[0;32m    217\u001b[0m request, expected_prefix, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_stream(\n\u001b[0;32m    218\u001b[0m     transaction,\n\u001b[0;32m    219\u001b[0m     retry,\n\u001b[0;32m    220\u001b[0m     timeout,\n\u001b[0;32m    221\u001b[0m )\n\u001b[1;32m--> 223\u001b[0m response_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_firestore_api\u001b[38;5;241m.\u001b[39mrun_query(\n\u001b[0;32m    224\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m    225\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_rpc_metadata,\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    227\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_iterator, expected_prefix\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\cloud\\firestore_v1\\services\\firestore\\client.py:1561\u001b[0m, in \u001b[0;36mFirestoreClient.run_query\u001b[1;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 1561\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\api_core\\retry\\retry_base.py:221\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m next_sleep \u001b[38;5;241m>\u001b[39m deadline:\n\u001b[0;32m    216\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    217\u001b[0m         error_list,\n\u001b[0;32m    218\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mTIMEOUT,\n\u001b[0;32m    219\u001b[0m         original_timeout,\n\u001b[0;32m    220\u001b[0m     )\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    222\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying due to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, sleeping \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(error_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], next_sleep)\n\u001b[0;32m    224\u001b[0m )\n",
      "\u001b[1;31mRetryError\u001b[0m: Timeout of 300.0s exceeded, last exception: 429 Quota exceeded."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache saved to disk.\n",
      "Cache saved to disk.\n",
      "Cache saved to disk.\n",
      "Cache saved to disk.\n",
      "Cache saved to disk.\n",
      "Cache saved to disk.\n",
      "Cache saved to disk.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def run_test_queries(use_cache):\n",
    "    test_queries = [\n",
    "        (\"767\", \"user\", \"2006-07-14\", \"2006-07-14\")\n",
    "    ]\n",
    "    times = []\n",
    "    \n",
    "    for query, query_type, start_date, end_date in test_queries:\n",
    "        start_time = time.time()\n",
    "        results = search_tweets(query, query_type, start_date, end_date, use_cache=use_cache)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "        print(f\"查询 '{query}' 类型为 '{query_type}' 耗时 {end_time - start_time:.2f} 秒，找到结果数: {len(results)}\")\n",
    "    \n",
    "    average_time = sum(times) / len(times)\n",
    "    print(f\"平均查询时间（{'启用' if use_cache else '禁用'}缓存）: {average_time:.2f} 秒\")\n",
    "\n",
    "# 首次运行测试，启用缓存\n",
    "run_test_queries(True)\n",
    "\n",
    "# 清除缓存，然后再次运行测试，禁用缓存\n",
    "cache.cache.clear()\n",
    "run_test_queries(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox, ttk, scrolledtext\n",
    "import psycopg2\n",
    "\n",
    "# Initialization of Firebase application\n",
    "cred = credentials.Certificate('D:/Download/twitter-a3b9a-firebase-adminsdk-b9pvo-f8f057cf01.json')\n",
    "db = firestore.client()\n",
    "\n",
    "def parse_twitter_date(datestr):\n",
    "    timezone = pytz.timezone(\"UTC\")  # Define your timezone\n",
    "    return datetime.strptime(datestr, '%a %b %d %H:%M:%S %z %Y').astimezone(timezone)\n",
    "\n",
    "def calculate_relevance(tweet, query):\n",
    "    text_score = query.lower() in tweet['text'].lower()\n",
    "    interaction_score = tweet.get('like_count', 0) + tweet.get('retweet_count', 0) + tweet.get('reply_count', 0)\n",
    "    return text_score * 0.5 + interaction_score * 0.5\n",
    "\n",
    "def get_user_data(user_ids):\n",
    "    if not user_ids:\n",
    "        return {}\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(host=\"localhost\", dbname=\"twitter_1\", user=\"postgres\", password=\"123456\", port=\"5432\")\n",
    "        cursor = conn.cursor()\n",
    "        format_strings = ','.join(['%s'] * len(user_ids))\n",
    "        cursor.execute(f\"SELECT id, name, screen_name, location, url FROM user_table WHERE id IN ({format_strings})\", tuple(user_ids))\n",
    "        user_data = {row[0]: {'name': row[1], 'screen_name': row[2], 'location': row[3], 'url': row[4]} for row in cursor.fetchall()}\n",
    "        return user_data\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to fetch user data: {e}\")\n",
    "        return {}\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def get_retweet_details(tweet_id):\n",
    "    retweets = db.collection('retweets').where('original_tweet_id', '==', tweet_id).stream()\n",
    "    user_ids = [retweet.to_dict()['retweeter_id'] for retweet in retweets]\n",
    "    users_info = get_user_data(user_ids)\n",
    "    retweet_info = []\n",
    "    for user_id in user_ids:\n",
    "        if user_id in users_info:\n",
    "            user_info = users_info[user_id]\n",
    "            retweet_info.append(f\"Retweeted by: {user_info['name']} at {user_info.get('retweet_time', 'Unknown')}\")\n",
    "        else:\n",
    "            retweet_info.append(\"Retweet details unavailable\")\n",
    "    return \"\\n\".join(retweet_info)\n",
    "\n",
    "def search_tweets(query, search_type=\"text\", start_date=None, end_date=None):\n",
    "    tweets = db.collection('tweets')\n",
    "    if search_type == \"text\":\n",
    "        tweets = tweets.where('text', '>=', query).where('text', '<=', query + '\\uf8ff')\n",
    "    elif search_type == \"hashtag\":\n",
    "        tweets = tweets.where('hashtags', 'array_contains', query)\n",
    "    results = []\n",
    "    if start_date and end_date:\n",
    "        utc_zone = pytz.utc\n",
    "        start_date = utc_zone.localize(datetime.strptime(start_date, \"%Y-%m-%d\"))\n",
    "        end_date = utc_zone.localize(datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1))\n",
    "        for doc in tweets.stream():\n",
    "            tweet = doc.to_dict()\n",
    "            tweet_created_at = parse_twitter_date(tweet['created_at'])\n",
    "            if start_date <= tweet_created_at <= end_date:\n",
    "                tweet['relevance_score'] = calculate_relevance(tweet, query)\n",
    "                results.append(tweet)\n",
    "        results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "    else:\n",
    "        results = [doc.to_dict() for doc in tweets.stream()]\n",
    "    user_ids = {result['user_id'] for result in results}\n",
    "    users_info = get_user_data(list(user_ids))\n",
    "    for result in results:\n",
    "        result['user'] = users_info.get(result['user_id'], {})\n",
    "        result['retweet_details'] = get_retweet_details(result['id_str'])\n",
    "    return results\n",
    "\n",
    "def perform_search():\n",
    "    search_query = query_entry.get()\n",
    "    search_type = search_type_combobox.get()\n",
    "    start_date = start_date_entry.get()\n",
    "    end_date = end_date_entry.get()\n",
    "    results = search_tweets(search_query, search_type, start_date, end_date)\n",
    "    results_display.delete(1.0, tk.END)\n",
    "    if not results:\n",
    "        messagebox.showinfo(\"Search Result\", \"No tweets found matching your criteria.\")\n",
    "        return\n",
    "    for index, result in enumerate(results):\n",
    "        user_info = result.get('user', {})\n",
    "        result_text = f\"{result['text']} - {user_info.get('name', 'Unknown')} - {result.get('created_at', 'No date')} - Retweets: {result.get('retweet_details', 'None')}\\n\"\n",
    "        results_display.insert(tk.END, result_text)\n",
    "        details_button = tk.Button(results_display, text=\"Details\", command=lambda r=result: show_details(r))\n",
    "        results_display.window_create(tk.END, window=details_button)\n",
    "        results_display.insert(tk.END, \"\\n\")\n",
    "\n",
    "def show_details(result):\n",
    "    user_info = result.get('user', {})\n",
    "    retweet_details = result.get('retweet_details', 'No retweet details available')\n",
    "    details = f\"Tweet ID: {result.get('id_str', 'N/A')}\\nUser: {user_info.get('name', 'Unknown')}\\nScreen Name: {user_info.get('screen_name', 'N/A')}\\nLocation: {user_info.get('location', 'N/A')}\\nText: {result['text']}\\nHashtags: {', '.join(result.get('hashtags', []))}\\nRetweet Details: {retweet_details}\"\n",
    "    messagebox.showinfo(\"Tweet Details\", details)\n",
    "\n",
    "# UI initialization and main loop\n",
    "root = tk.Tk()\n",
    "root.title(\"Twitter Search App\")\n",
    "query_label = tk.Label(root, text=\"Enter search query:\")\n",
    "query_label.pack()\n",
    "query_entry = tk.Entry(root)\n",
    "query_entry.pack()\n",
    "search_type_label = tk.Label(root, text=\"Search by:\")\n",
    "search_type_label.pack()\n",
    "search_type_combobox = ttk.Combobox(root, values=(\"text\", \"hashtag\", \"user\"))\n",
    "search_type_combobox.pack()\n",
    "search_type_combobox.current(0)\n",
    "start_date_label = tk.Label(root, text=\"Start Date (YYYY-MM-DD):\")\n",
    "start_date_label.pack()\n",
    "start_date_entry = tk.Entry(root)\n",
    "start_date_entry.pack()\n",
    "end_date_label = tk.Label(root, text=\"End Date (YYYY-MM-DD):\")\n",
    "end_date_label.pack()\n",
    "end_date_entry = tk.Entry(root)\n",
    "end_date_entry.pack()\n",
    "search_button = tk.Button(root, text=\"Search\", command=perform_search)\n",
    "search_button.pack()\n",
    "results_display = scrolledtext.ScrolledText(root, wrap=tk.WORD, height=10, width=50)\n",
    "results_display.pack(pady=20)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e09c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox, ttk, scrolledtext\n",
    "import psycopg2\n",
    "\n",
    "# Initialization of Firebase application\n",
    "cred_path = 'D:/Download/twitter-a3b9a-firebase-adminsdk-b9pvo-f8f057cf01.json'\n",
    "cred = credentials.Certificate(cred_path)\n",
    "db = firestore.client()\n",
    "\n",
    "def parse_twitter_date(datestr):\n",
    "    return datetime.strptime(datestr, '%a %b %d %H:%M:%S %z %Y').astimezone(pytz.UTC)\n",
    "\n",
    "def calculate_relevance(tweet, query):\n",
    "    text_score = query.lower() in tweet['text'].lower()\n",
    "    interaction_score = tweet.get('like_count', 0) + tweet.get('retweet_count', 0) + tweet.get('reply_count', 0)\n",
    "    return text_score * 0.5 + interaction_score * 0.5\n",
    "\n",
    "def get_user_data(user_ids):\n",
    "    if not user_ids:\n",
    "        return {}\n",
    "    try:\n",
    "        conn = psycopg2.connect(host=\"localhost\", dbname=\"twitter_1\", user=\"postgres\", password=\"123456\", port=\"5432\")\n",
    "        cursor = conn.cursor()\n",
    "        format_strings = ','.join(['%s'] * len(user_ids))\n",
    "        query = f\"SELECT id, name, screen_name, location, url FROM user_table WHERE id IN ({format_strings})\"\n",
    "        cursor.execute(query, tuple(user_ids))\n",
    "        user_data = {row[0]: {'name': row[1], 'screen_name': row[2], 'location': row[3], 'url': row[4]} for row in cursor.fetchall()}\n",
    "    except psycopg2.Error as e:\n",
    "        messagebox.showerror(\"Database Error\", f\"Failed to fetch user data: {e}\")\n",
    "        return {}\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return user_data\n",
    "\n",
    "def get_retweet_details(tweet_id):\n",
    "    try:\n",
    "        retweets = db.collection('retweets').where('original_tweet_id', '==', tweet_id).stream()\n",
    "        user_ids = [retweet.to_dict()['retweeter_id'] for retweet in retweets]\n",
    "        users_info = get_user_data(user_ids)\n",
    "        retweet_info = [f\"Retweeted by: {users_info[user_id]['name']} at {users_info[user_id].get('retweet_time', 'Unknown')}\"\n",
    "                        if user_id in users_info else \"Retweet details unavailable\" for user_id in user_ids]\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Firebase Error\", f\"Failed to fetch retweet details: {e}\")\n",
    "        return [\"Error retrieving retweet details\"]\n",
    "    return \"\\n\".join(retweet_info)\n",
    "\n",
    "def search_tweets(query, search_type=\"text\", start_date=None, end_date=None):\n",
    "    try:\n",
    "        tweets = db.collection('tweets')\n",
    "        if search_type == \"text\":\n",
    "            tweets = tweets.where('text', '>=', query).where('text', '<=', query + '\\uf8ff')\n",
    "        elif search_type == \"hashtag\":\n",
    "            tweets = tweets.where('hashtags', 'array_contains', query)\n",
    "        if start_date and end_date:\n",
    "            start_date, end_date = [pytz.utc.localize(datetime.strptime(d, \"%Y-%m-%d\")) for d in [start_date, end_date]]\n",
    "            end_date += timedelta(days=1)\n",
    "            tweets = tweets.where('created_at', '>=', start_date).where('created_at', '<=', end_date)\n",
    "        tweets = tweets.stream()\n",
    "        results = [{**doc.to_dict(), 'relevance_score': calculate_relevance(doc.to_dict(), query)} for doc in tweets]\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Firebase Error\", f\"Failed to fetch tweets: {e}\")\n",
    "        return []\n",
    "    results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "    return results\n",
    "\n",
    "# UI initialization and main loop\n",
    "def setup_ui():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Twitter Search App\")\n",
    "    elements = {\n",
    "        \"query\": tk.Entry(root),\n",
    "        \"search_type\": ttk.Combobox(root, values=(\"text\", \"hashtag\", \"user\"), state=\"readonly\"),\n",
    "        \"start_date\": tk.Entry(root),\n",
    "        \"end_date\": tk.Entry(root),\n",
    "        \"results_display\": scrolledtext.ScrolledText(root, wrap=tk.WORD, height=10, width=50)\n",
    "    }\n",
    "    elements[\"search_type\"].current(0)\n",
    "    for text, widget in [(\"Enter search query:\", elements[\"query\"]), (\"Search by:\", elements[\"search_type\"]),\n",
    "                         (\"Start Date (YYYY-MM-DD):\", elements[\"start_date\"]), (\"End Date (YYYY-MM-DD):\", elements[\"end_date\"])]:\n",
    "        tk.Label(root, text=text).pack()\n",
    "        widget.pack()\n",
    "    tk.Button(root, text=\"Search\", command=lambda: perform_search(elements)).pack()\n",
    "    elements[\"results_display\"].pack(pady=20)\n",
    "    root.mainloop()\n",
    "\n",
    "def perform_search(elements):\n",
    "    query = elements[\"query\"].get()\n",
    "    search_type = elements[\"search_type\"].get()\n",
    "    start_date = elements[\"start_date\"].get()\n",
    "    end_date = elements[\"end_date\"].get()\n",
    "    results = search_tweets(query, search_type, start_date, end_date)\n",
    "    results_display = elements[\"results_display\"]\n",
    "    results_display.delete(1.0, tk.END)\n",
    "    if not results:\n",
    "        messagebox.showinfo(\"Search Result\", \"No tweets found matching your criteria.\")\n",
    "    for result in results:\n",
    "        user_info = result.get('user', {})\n",
    "        result_text = f\"{result['text']} - {user_info.get('name', 'Unknown')} - {result.get('created_at', 'No date')} - Retweets: {result.get('retweet_details', 'None')}\\n\"\n",
    "        results_display.insert(tk.END, result_text)\n",
    "\n",
    "setup_ui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f20014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
